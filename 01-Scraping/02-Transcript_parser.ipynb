{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Transcript Parser\n",
    "In the end of the previous notebook, we were able to retrieve the raw *Transcript* field from the Parliament website. However, those are formatted in a very unconvenient way, having a lot of HTML tags (e.g. `<pd_text>` or `<p>`). This is why we need to parse it all to get a usable text. \n",
    "\n",
    "Before actually parsing, let us do the usual imports\n",
    "## 0. Usual Imports\n",
    "\n",
    "To install : \n",
    "\n",
    "```\n",
    "pip install PyPDF2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import logging\n",
    "import gensim\n",
    "import bz2\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing the Transcripts to a usable form\n",
    "First of all, it is important to note that this code **TAKES AN EXTREMELY LONG TIME TO BE RUN**. We would strongly advise you not to run it, as the results are already available in the *path* provided below. The raw *Transcript*  data can be found in the *Transcript Copy* folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../datas/scrap/Transcript/'\n",
    "allFiles = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "# Iterate over all the files in the Transcript folder \n",
    "# For each of them, import the csv file, iterate over the entries and find the Text\n",
    "for file_ in allFiles:\n",
    "    print(file_)\n",
    "    data = pd.read_csv(file_,index_col=False)\n",
    "    for index, row in data.iterrows():\n",
    "        # This line is necessary to remove the NaN entries in the Text fields.\n",
    "        # Otherwise, the parsing would crash\n",
    "        if data.Text[index] == data.Text[index] : \n",
    "            data.Text[index]= BeautifulSoup(data.Text[index],'html.parser').text.replace('\\n',' ')\n",
    "            \n",
    "    #At the end, overwrite the file with the parsed data.        \n",
    "    data.to_csv(file_,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
