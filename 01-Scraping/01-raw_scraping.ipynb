{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Raw Scraping\n",
    "\n",
    "What we do here is the scraping of the Swiss Parliament website from the hierarchy described in the [metadata]() we were given. We were able to create a typical URL from which we start our queries for the different fields, as we will describe below. This URL is saved in the *base_url.txt* and has the form \n",
    "\n",
    "```\n",
    "https://ws.parlament.ch/odata.svc/[]?$filter=()%20gt%20{0}L%20and%20()%20lt%20{1}L%20and%20Language%20eq%20%27FR%27\n",
    "```\n",
    "\n",
    "with a lot of missing fields that will be completed for a specific query further down. \n",
    "\n",
    "## 0. Usual Imports\n",
    "A lot of libraries are required to successfully scrap the data and put them to csv files. To install\n",
    "\n",
    "```\n",
    "pip install xmljson\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import html5lib \n",
    "from lxml import *\n",
    "import numpy as np\n",
    "import xmljson \n",
    "from xmljson import badgerfish as bf\n",
    "from json import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO\n",
    "import webbrowser\n",
    "import requests\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key Parsing Functions\n",
    "Before starting, we just check whether the path we will use as our data folder exists. If it does, we store it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../datas\"):\n",
    "    os.makedirs(\"../datas\")\n",
    "if not os.path.exists(\"../datas/scrap\"):\n",
    "    os.makedirs(\"../datas/scrap\")\n",
    "directory = '../datas/scrap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parse_data` is the first parsing function that we use. Given a completed url, it fetches a dictionary which contains everything related to a field, and formats it into a DataFrame. For instance, given a get query associated to the *LegislativePeriod*, it will get all the fields in it (e.g. *ID*, *Language*, *LegislativePeriodNumber*, ...), then format it in a DataFrame where the ID will be the row, and the other fields the column.\n",
    "\n",
    "**Important note : ** The *Language* field, present in every field, will systematically be filtered with the French entries (*FR*), because the only thing that it changes is for instance the name of the partys that will be in German instead of french and so on. The same goes for the other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_data(base_url) :\n",
    "    \"\"\"\n",
    "        Fetches and parses data from the base_url given as parameter. Then formats it into a DataFrame\n",
    "        which is returned. The quadruple for loop is due to the particular form of the website. \n",
    "        @param base_url : the precise get request we want to formulate.\n",
    "        @return data : a DataFrame which contains the formatted result from the query.\n",
    "    \"\"\"\n",
    "    \n",
    "    with urllib.request.urlopen(base_url) as url:\n",
    "        s = url.read()\n",
    "    \n",
    "    root = ET.fromstring(s)\n",
    "    \n",
    "    dict_ = {}\n",
    "    base = \"{http://www.w3.org/2005/Atom}\"\n",
    "    for child in root.iter(base+'entry'):\n",
    "        for children in child.iter(base+'content') :\n",
    "            for properties in children :\n",
    "                for subject in properties : \n",
    "                        #print(subject.text)\n",
    "                    s = subject.tag.split('}')\n",
    "                    if s[1] in dict_ :\n",
    "                        dict_[s[1]].append(subject.text)\n",
    "                    else : \n",
    "                        dict_[s[1]] = [subject.text]\n",
    "    data = pd.DataFrame(dict_)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`save_data` is our most important function here. It allows us to form the url with which we want to make our query, then calls the `parse_data` function and finally stores the resuting DataFrame to the corresponding directory. \n",
    "\n",
    "TODO : describe why the completing is done like that\n",
    "TODO : understand the id\\_ \"game\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_data(parent_id, directory, id_name, parent_name = None, subject = None, url = None) :\n",
    "    \"\"\"\n",
    "        Forms the correct url to use to query the website. \n",
    "        Then, fetches the data and parses them into a usable csv file.\n",
    "        @param parent_id :  the ID of the parent field \n",
    "                            (i.e. the last one we parsed and from which we got the ID \n",
    "                            -> necessary to make the query on the correct IDs) \n",
    "        @param directory : the directory in which the parsing will be saved\n",
    "        @param id_name : the name of the parent field for exporting purposes\n",
    "        @param parent_name : the name of the parent field formatted for the query\n",
    "        @param subject : The topic we're currently parsing.\n",
    "        @param url :   the specific url for the topic we're treating, if we need a special one \n",
    "                       (otherwise we load the base_url)\n",
    "        @return index : the indices on which our data range\n",
    "        @return data : the formatted DataFrame containing all the infos that were scraped.\n",
    "    \"\"\"\n",
    "    if url == None :\n",
    "        with open('base_url.txt', 'r') as myfile:\n",
    "            url=myfile.read()\n",
    "    if subject != None :\n",
    "        url = url.replace('[]',subject)\n",
    "    if parent_name != None :   \n",
    "        url = url.replace('()',parent_name)\n",
    "    url = url.replace('{0}',str(np.maximum(min(parent_id)-1,0)))\n",
    "    url = url.replace('{1}',str(max(parent_id)+1))\n",
    "    print(url)\n",
    "    \n",
    "    data = parse_data(url)\n",
    "    \n",
    "    # The website might return empty data. In the case where that happens, we return nothing.\n",
    "    # In the case where useful data is returned, we save it to a specific location with a name given\n",
    "    # by the parameters we passed.\n",
    "    if not data.empty :\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        index = list(map(int, data['ID'].unique().tolist()))\n",
    "        data.to_csv(directory+'/'+id_name+ 'id_'+str(min(parent_id))+'-'+str(max(parent_id))+'.csv')\n",
    "        return index ,data\n",
    "    else :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actual Parsing of the Data\n",
    "Now that our parsing functions were defined, we can use them to retrieve the data we need from the website. Every cell will perform the query for one specific field, and the data we retrieve will allow us to go further down into the data tree (cf. the visualisation of the hierarchy of the data with [XOData](http://pragmatiqa.com/xodata/#)) and retrieve the fields we are interested with. We especially need to get the *Transcripts* and *Voting* fields, as the first describes all the discussions that happen during a session at the Parliament and the second all the results of the votes that happen during a session. Those are the two essential components to the machine learning we'll do later on. We process from the highest \"branch\" of the tree of data, namely the Legislative Period. This is the year during which the Parliament met, which each time two sessions. \n",
    "\n",
    "### 2.1. Saving Legislative Data\n",
    "Every cell will roughly work the same, we give first the specific link to access the field if it requires it (otherwise we use the base_url that we described before). We set the directory in which we save our data and then retrieve it. We need to retrieve the IDs from the Legislative Period in order to be able to keep on querying, as querying with an invalid Legislative Period ID will lead to the crashing of the request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "legislative_url =\"https://ws.parlament.ch/odata.svc/LegislativePeriod?$filter=LegislativePeriodNumber%20gt%20{0}%20and%20LegislativePeriodNumber%20lt%20{1}\"\n",
    "base_legi_directory = directory+ \"/legi\"\n",
    "legi_periode_id, _  = save_data([0,100], base_legi_directory,'legi',None,None,legislative_url)\n",
    "print(legi_periode_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the IDs are continuous, ranging from 37 to 50. We will then query all the data which have their LegislativePeriodID between those two bounds. The first line gives us the link we build to make the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Saving Vote Data\n",
    "A close field that we can access from the LegislativePeriod is the *Vote* field. It is the interface between an object that is voted at the parliament at a specific time and the results of this vote, which are available in the *Voting* field. We will then query all the votes that happened for the given *Legislative Periods* we're considering. We see below that the IDs range from 1 to 17983, which isn't unique as an object will be voted several times, go between the National Council and the State Council iteratively until the issue is accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_vote_directory= directory + \"/Vote\"\n",
    "vote_id , _ = save_data(legi_periode_id,base_vote_directory,'legi','IdLegislativePeriod','Vote')\n",
    "print(str(min(vote_id))+' '+str(max(vote_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Saving Session Data\n",
    "The *Session* field helps us identify precisely when an object was voted. It is due to the fact that, for a given *Legislative Period*, which basically is a year, there are several *Session*s, usually a winter and a summer one, and there might be some special ones as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_session_directory= directory+ \"/Session\"\n",
    "session_id, _ = save_data(legi_periode_id,base_session_directory,'Legi','LegislativePeriodNumber','Session')\n",
    "print(str(min(session_id))+' '+str(max(session_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Saving Voting Data.\n",
    "The routine below is not very efficient and is operated manually, being ran several times in order to obtain all the *Voting* items. **This is why it shouldn't be ran just in its current state**. The complications come from the fact that are the *Voting* IDs aren't contiguous, and that querying an unexisting ID will make the query crash. Moreover, we can only query the IDs two by two, we would otherwise receive a timeout as a response. This is why we need to do the following. It is due to the fact that each ID encapsulates a lot of Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_voting_directory= directory+ \"/Voting\"\n",
    "\n",
    "# Iterate over some specific range ot data\n",
    "for i in range(np.int16((5005-4811)/2)+1) :\n",
    "    # Particular URL to get the Voting field from.\n",
    "    url =\"https://ws.parlament.ch/odata.svc/Voting/$count?$filter=Language%20eq%20%27FR%27%20and%20IdSession%20ge%20[1]%20and%20IdSession%20le%20[2]\"\n",
    "    session_id.sort()\n",
    "    \n",
    "    # ID of the Voting we query\n",
    "    id_ = 4811+2*i\n",
    "    \n",
    "    # Query items two by two\n",
    "    take_id = [id_,id_+1]\n",
    "    url = url.replace('[1]',str(min(take_id)))\n",
    "    url = url.replace('[2]',str(max(take_id)))\n",
    "    with urllib.request.urlopen(url) as url:\n",
    "        s = url.read()\n",
    "    print(\"count equals ====>\" + str(s))\n",
    "    vote_id , _ = save_data(take_id,base_voting_directory,'Session','IdSession','Voting')\n",
    "print(str(min(vote_id))+' '+str(max(vote_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Saving Meeting Data.\n",
    "This field depends on the session, and records every *Meeting* that each chambers of the parliament has during a session. It is necessary for us to have it in order to be able to access the *Transcript* field later, which is the transcription of every *Subject* that is discussed during a *Meeting* of any of the chambers during a *Session* of a *Legislative Period*. Nothing very surprising here. We access it from the *Session* field, that's why we need to record the *session_id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_transcript_directory= directory+ \"/Meeting\"\n",
    "meeting_id,_ = save_data(session_id,base_transcript_directory,'Session','IdSession','Meeting')\n",
    "print(str(min(meeting_id))+' '+str(max(meeting_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Saving Subject Data.\n",
    "The field, as we described above, contains all the *Subject*s which are discussed during a single *Meeting*, and we hence need the *meeting_id* list to be able to query it. It is the last step before being able to access to the much desired *Transcript* data and retrieve it in a coherent way.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_subject_directory= directory+ \"/Subject\"\n",
    "subject_id, _ = save_data(meeting_id,base_subject_directory,'Meeting','IdMeeting','Subject')\n",
    "print(str(min(Subject_id))+' '+str(max(Subject_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Saving Transcript Data\n",
    "Now that we have the list of the *Subject_id*, we are finally able to get the *Transcript* field, a record that everything discussed at the parliament, on which we will base our Natural Language Processing Analysis later on. The query is a bit complicated.\n",
    "\n",
    "*TODO : Explain why*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_transcript_directory= directory+ \"/Transcript\"\n",
    "max_transcript_id = 206649\n",
    "transcript_id = [0]\n",
    "while max(transcript) < max_transcript_id :\n",
    "    transcript, transcript = save_data(subject_id,base_transcript_directory,'Subject','IdSubject','Transcript')\n",
    "    max_id = max(list(map(int,transcript['IdSubject'])))\n",
    "    subject_id = [i for i in subject_id if i > max_id]\n",
    "    print(str(min(transcript))+' '+str(max(transcript)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ws.parlament.ch/odata.svc/MemberCouncil?$filter=ID%20gt%204999L%20and%20ID%20lt%206001L%20and%20Language%20eq%20%27FR%27\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9ceab321f36e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlegi_periode_id\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlegi_periode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegi_periode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegi_periode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_legi_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MemberCouncil'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MemberCouncil'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlegislative_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#print(legi_periode_id)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "legislative_url =\"https://ws.parlament.ch/odata.svc/[]?$filter=()%20gt%20{0}L%20and%20()%20lt%20{1}L%20and%20Language%20eq%20%27FR%27\"\n",
    "base_legi_directory = directory+ \"/MemberCouncil\"\n",
    "legi_periode_id =[5000]\n",
    "for i in range(10):\n",
    "    legi_periode_id, _  = save_data([max(legi_periode_id),max(legi_periode_id)+1000], base_legi_directory,'MemberCouncil','ID','MemberCouncil',legislative_url)\n",
    "    #print(legi_periode_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
