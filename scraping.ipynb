{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import html5lib \n",
    "from lxml import *\n",
    "import numpy as np\n",
    "import xmljson \n",
    "from xmljson import badgerfish as bf\n",
    "from json import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO\n",
    "import webbrowser\n",
    "import requests\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"datas\"):\n",
    "    os.makedirs(\"datas\")\n",
    "directory = 'datas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsing_datas(base_url) :\n",
    "    with urllib.request.urlopen(base_url) as url:\n",
    "        s = url.read()\n",
    "    root = ET.fromstring(s)\n",
    "\n",
    "    dict_ = {}\n",
    "    base = \"{http://www.w3.org/2005/Atom}\"\n",
    "    for child in root.iter(base+'entry'):\n",
    "        for children in child.iter(base+'content') :\n",
    "            for properties in children :\n",
    "                for subject in properties : \n",
    "                        #print(subject.text)\n",
    "                    s = subject.tag.split('}')\n",
    "                    if s[1] in dict_ :\n",
    "                        dict_[s[1]].append(subject.text)\n",
    "                    else : \n",
    "                        dict_[s[1]] = [subject.text]\n",
    "    data = pd.DataFrame(dict_)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving_datas definition function \n",
    "- return ids of of session/vote .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_datas(url, paths, paths_id,directory,final_index,change_url = False) :\n",
    "        if len(paths_id[paths[0]]) == 0 :\n",
    "            \n",
    "            data = parsing_datas(url)\n",
    "            if not data.empty :\n",
    "\n",
    "                directory += '/'+ paths[0]\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                id_  = data['ID'].unique()[1:].tolist()\n",
    "                final_index = final_index+id_\n",
    "                data.to_csv(directory+'/'+paths[-1]+\".csv\")\n",
    "                \n",
    "        else :\n",
    "            \n",
    "            for index in paths_id[paths[0]] :\n",
    "                new_directory = directory\n",
    "                new_paths = paths_id.copy()\n",
    "                new_paths.pop(paths[0],None)\n",
    "                new_directory += '/'+paths[0]+'_id_'+str(index)\n",
    "                if not os.path.exists(new_directory):\n",
    "                    os.makedirs(new_directory)\n",
    "                new_url = url\n",
    "                new_change_url = change_url\n",
    "                if change_url and len(paths)==2 :\n",
    "                    new_url = new_url.replace('{}',index)  \n",
    "                    new_change_url = False\n",
    "                    print(new_url)\n",
    "                final_index = save_data(new_url,paths[1:],new_paths,new_directory,final_index,new_change_url)\n",
    "        return final_index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_data(id_,directory,id_name,parent_id= None,subject=None, url = None) :\n",
    "    if url == None :\n",
    "        with open('base_url.txt', 'r') as myfile:\n",
    "            url=myfile.read()\n",
    "    if subject != None :\n",
    "        url = url.replace('[]',subject)\n",
    "    if parent_id != None :   \n",
    "        url = url.replace('()',parent_id)\n",
    "    url = url.replace('{0}',str(np.maximum(min(id_)-1,0)))\n",
    "    url = url.replace('{1}',str(max(id_)+1))\n",
    "    print(url)\n",
    "    data = parsing_datas(url)\n",
    "    if not data.empty :\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        index = list(map(int, data['ID'].unique().tolist()))\n",
    "        data.to_csv(directory+'/'+id_name+ 'id_'+str(min(id_))+'-'+str(max(id_))+'.csv')\n",
    "        return index ,data\n",
    "    else :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving legislative datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "legislative_url =\"https://ws.parlament.ch/odata.svc/LegislativePeriod?$filter=LegislativePeriodNumber%20gt%20{0}%20and%20LegislativePeriodNumber%20lt%20{1}\"\n",
    "base_legi_directory = \"datas/legi\"\n",
    "legi_periode_id, _  = save_data([0,100], base_legi_directory,'legi',None,None,legislative_url)\n",
    "print(legi_periode_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving vote datas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_vote_directory= \"datas/Vote\"\n",
    "vote_id , _ = save_data(legi_periode_id,base_vote_directory,'legi','IdLegislativePeriod','Vote')\n",
    "print(str(min(vote_id))+' '+str(max(vote_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_session_directory= \"datas/Session\"\n",
    "session_id, _ = save_data(legi_periode_id,base_session_directory,'Legi','LegislativePeriodNumber','Session')\n",
    "print(str(min(session_id))+' '+str(max(session_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_transcript_directory= \"datas/Meeting\"\n",
    "meeting_id,_ = save_data(session_id,base_transcript_directory,'Session','IdSession','Meeting')\n",
    "print(str(min(meeting_id))+' '+str(max(meeting_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_subject_directory= \"datas/Subject\"\n",
    "Subject_id, _ = save_data(meeting_id,base_subject_directory,'Meeting','IdMeeting','Subject')\n",
    "print(str(min(Subject_id))+' '+str(max(Subject_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_transcript_directory= \"datas/Transcript\"\n",
    "max_Transcript_id = 206649\n",
    "Transcript_id = [0]\n",
    "while max(Transcript_id) < max_Transcript_id :\n",
    "    Transcript_id, Transcript = save_data(Subject_id,base_transcript_directory,'Subject','IdSubject','Transcript')\n",
    "    max_id = max(list(map(int,Transcript['IdSubject'])))\n",
    "    Subject_id = [i for i in Subject_id if i > max_id]\n",
    "    print(str(min(Transcript_id))+' '+str(max(Transcript_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
