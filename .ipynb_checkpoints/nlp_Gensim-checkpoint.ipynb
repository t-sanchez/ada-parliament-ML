{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Topic Modelling using the Gensim Library\n",
    "\n",
    "Usual imports come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import logging\n",
    "import gensim\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data from the Transcript files\n",
    "At the moment, we only consider the entries for which the field `LanguageOfText` is `FR`, namely the ones in French. We will consider the text in German later on. We show below one example of the text we consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "path = 'datas/Transcript/'\n",
    "allFiles = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "for file_ in allFiles:\n",
    "    data = pd.read_csv(file_)\n",
    "    dataset = dataset + list(data[(data['Text'] == data['Text']) & (data['LanguageOfText'] == 'FR')]['Text'].values)\n",
    "    \n",
    "print('Length of the dataset', len(dataset))\n",
    "print(dataset[0],'\\n',dataset[1],'\\n',dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the transcripts largely vary from an entry to another, but it reflects exactly what is discussed at the federal parliament. Processing them correctly will allow us to grasp the topic which are discussed at the parliament."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Format the data in order to use LDA with Gensim\n",
    "First of all, we load the `stop_words`, a list which refers all the common words for French, and that we must not take into accoung when doing the topic modelling, as they do not convey any useful information. The pipeline we follow is the following :\n",
    " 1. Load the `stop_words`\n",
    " 2. Remove those common words and tokenize our dataset (break it down into words) \n",
    " 3. We count the frequency of the words and remove the ones that appear only once in total.\n",
    " 4. **TODO - ** Implement the *Stemming* of the data (cf. [a French stemming algorithm](http://snowball.tartarus.org/algorithms/french/stemmer.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('fr')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## First of all we load the stop_words list\n",
    "import re\n",
    "\n",
    "with open (\"French_stop_words_changed.txt\", \"r\") as myfile:\n",
    "    stop_words=myfile.read()  \n",
    "stop_words = stop_words.split(',')\n",
    "## Secondly we remove the common words in our document corpus and tokenize \n",
    "# The re.split function takes as first arguments everything we split at. At the moment, this is \n",
    "# ' ' - '\\' - ''' (apostrophe) -  '\\n' - '(', ')' - ',' - '.' - ':' - ';'\n",
    "texts = [[word for word in re.split(' |\\'|\\n|\\(|\\)|,|;|:|\\.',document.lower()) if word not in stop_words]\n",
    "         for document in dataset]\n",
    "\n",
    "# Thirdly we remove the words that appear only once in a text\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "    \n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "print(texts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perform the LDA topic modelling and print the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the data into a dictionnary and a corpus, necessary entries for the LdaModel function of Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
