{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Machine Learning on the Votations\n",
    "What we aim to perform now is predict the topics that are treated in a *Vote*, given the short string description of the title of the law (*BillTitle*) and the *BusinessTitle*. We apply our model, saved at `../datas/lda/ldamodel` to the data from the *Voting* field, in order to prepare it for the machine learning we'll do later on.\n",
    "\n",
    "## 0. Usual Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import logging\n",
    "import gensim\n",
    "import bz2\n",
    "import re\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. initialisation of function for topic determination\n",
    "First of all, we define a function, *getTopicForQuery* in order to obtain the topics probability dsistribution for the *lda* model we're currently using. It will be of use mostly to retrieve the topic probability distribution for the attributes from the merged  *BillTitle* and *BusinessTitle*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopicForQuery (question,stoplist,dictionary,lda):\n",
    "    \"\"\"\n",
    "        Returns the topic probability distribution for a given input question, filtering with the stoplist \n",
    "        and finding the matches in the dictionary of words we have from our topic modelling algorithm.\n",
    "        @param question : The string from which we want to extract the topic\n",
    "        @param stoplist : The list of common words for the language, that we want to exclude\n",
    "        @param dictionary : The dictionary of all the words we find for a given lda model (associated to lda)\n",
    "        @param lda : the model of lda (Latent Dirichlet Allocation) that we want to model the topics from.\n",
    "        @return the topic probability distribution for the given question\n",
    "    \"\"\"\n",
    "    # 1. Question -> Lower case -> Split -> Exclude common words\n",
    "    temp = question.lower()\n",
    "    words = re.findall(r'\\w+', temp, flags = re.UNICODE | re.LOCALE)\n",
    "    important_words = []\n",
    "    important_words = filter(lambda x: x not in stoplist, words)\n",
    "\n",
    "    # 2. Find matches in the dictionary of words and get the topics\n",
    "    ques_vec = []\n",
    "    ques_vec = dictionary.doc2bow(important_words)\n",
    "    \n",
    "    return ldamodel.get_document_topics(ques_vec,minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the lda model we use along with the stop words, in order to have them available for the time we will use them, avoiding to reload them every time we call *getTopicForQuery*. We also load our *lda* model for once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words_de = get_stop_words('de')\n",
    "\n",
    "with open (\"../datas/stop_dictionaries/French_stop_words_changed.txt\", \"r\") as myfile:\n",
    "    stop_words=myfile.read()  \n",
    "stop_words = stop_words.split(',')\n",
    "\n",
    "stop_words = stop_words_de+stop_words\n",
    "ldamodel = gensim.models.LdaModel.load('../datas/lda/ldamodel', mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation of the Voting DataFrame\n",
    "We load the Voting DataFrame, take only the relevant fields for us and add the topic probability distribution before exporting it. It will be ready for our Machine Learning algorithm later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_tmp = []\n",
    "path = '../datas/scrap/Voting'\n",
    "allFiles = glob.glob(os.path.join(path, 'Session*.csv'))\n",
    "\n",
    "for file_ in allFiles:\n",
    "    print(file_)\n",
    "    data_tmp = pd.read_csv(file_)\n",
    "    dataset_tmp += [data_tmp] \n",
    "data_frame = pd.concat(dataset_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 slection of interesting fields in data_frame\n",
    "We take only the relevant fields to us, that is \n",
    "- *BillTitle* : The name of the voted law\n",
    "- *BusinessTitle* : The description of what is talked about at the parliament\n",
    "- *FirstName* and *LastName* : The name of the persone voting\n",
    "- *Decision* : The vote of the person\n",
    "- *text* : a field which combine *BillTitle* and *BusinessTitle*, we will have a better NLP clustering using this field with our LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "votation_frame = data_frame[['BillTitle','BusinessTitle','FirstName','LastName','Decision']]\n",
    "votation_frame = votation_frame.fillna(value='')\n",
    "votation_frame['text'] = votation_frame['BillTitle']+' '+votation_frame['BusinessTitle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a smaller DataFrame which contains only the subjects that are voted, we do not repeat the text each time for each person who votes. Hence we will perform the NLP once on each unique entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_dict = {'text': votation_frame.text.unique()}\n",
    "topic_learning_frame = pd.DataFrame(text_dict)\n",
    "topic_learning_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Topic Clustering\n",
    "\n",
    "We define first the function *insert_topic*, which creates a dictionary containing the topics for all texts in a *data_frame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def insert_topic(data_frame) :\n",
    "    dict_ = {}\n",
    "    dict_['text'] =data_frame['text'].values\n",
    "    with open (\"datas/lda/topics.txt\", \"r\") as myfile:\n",
    "        s=myfile.read()  \n",
    "    topics = s.split('\\n')\n",
    "    topics_dic = {}\n",
    "    for topic in topics :\n",
    "        if len(topic)>1 :\n",
    "            name = topic.split(':')\n",
    "        topics_dic[name[0]] = name[1]\n",
    "    dictionary = gensim.corpora.Dictionary.load('datas/lda/ldaDictionary')\n",
    "    for index, text in zip(data_frame.index,data_frame['text'].values) :\n",
    "        if index%1000 == 0 :\n",
    "             print(index)\n",
    "        for topic in getTopicForQuery(text,stop_words,dictionary,ldamodel) :\n",
    "            if (topics_dic[str(topic[0])]) in dict_ :\n",
    "                dict_[topics_dic[str(topic[0])]] +=[topic[1]]\n",
    "            else :\n",
    "                dict_[topics_dic[str(topic[0])]] =[topic[1]]\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../datas/nlp_results\"):\n",
    "    os.makedirs(\"./datas/nlp_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a frame using the topics found using *insert_topic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics_dict = insert_topic(topic_learning_frame)\n",
    "topics_frame = pd.DataFrame(topics_dict)\n",
    "topics_frame.to_csv('../datas/nlp_results/voting_with_topics_unique.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally merging the topics with the original frame containing the name and decision of parlementeer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(pd.merge(votation_frame,topics_frame)).to_csv('../datas/nlp_results/voting_with_topics.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
