{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Topic Modelling using the Gensim Library\n",
    "\n",
    "Usual imports come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import logging\n",
    "import gensim\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data from the Transcript files\n",
    "At the moment, we only consider the entries for which the field `LanguageOfText` is `FR`, namely the ones in French. We will consider the text in German later on. We show below one example of the text we consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "path = '../datas/treated_data/Transcript/'\n",
    "#path = 'datas/Vote/'\n",
    "allFiles = glob.glob(os.path.join(path, 'FR*.csv'))\n",
    "\n",
    "for file_ in allFiles:\n",
    "    data = pd.read_csv(file_)\n",
    "    dataset = dataset + list(data[(data['Text'] == data['Text'])]['Text'].values)\n",
    "    #dataset = dataset + list(data[(data['BusinessTitle'] == data['BusinessTitle'])]['BusinessTitle'].values+' ')\n",
    "\n",
    "    \n",
    "print('Length of the dataset', len(dataset))\n",
    "#print(dataset[0],'\\n',dataset[1])\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the transcripts largely vary from an entry to another, but it reflects exactly what is discussed at the federal parliament. Processing them correctly will allow us to grasp the topic which are discussed at the parliament."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Format the data in order to use LDA with Gensim\n",
    "First of all, we load the `stop_words`, a list which refers all the common words for French, and that we must not take into accoung when doing the topic modelling, as they do not convey any useful information. The pipeline we follow is the following :\n",
    " 1. Load the `stop_words`\n",
    " 2. Remove those common words and tokenize our dataset (break it down into words) \n",
    " 3. We count the frequency of the words and remove the ones that appear only once in total.\n",
    " 4. (Implement the *Stemming* of the data (cf. [a French stemming algorithm](http://snowball.tartarus.org/algorithms/french/stemmer.html)). (Done with the [nltk](http://www.nltk.org/api/nltk.stem.html) library) ) -> Not implemented at the moment\n",
    " 5.  Remove all the words of length <= 2.\n",
    " \n",
    " **N.B. THIS ALGORITHM IS VERY SLOW !!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## First of all we load the stop_words list\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# German stop words because there are some german words even in the french transcript\n",
    "stop_words_de = get_stop_words('de')\n",
    "\n",
    "# Loading the custom french stop-words list\n",
    "with open (\"stop_dictionaries/French_stop_words_changed.txt\", \"r\") as myfile:\n",
    "    stop_words=myfile.read()  \n",
    "stop_words = stop_words.split(',')\n",
    "\n",
    "stop_words = stop_words_de+stop_words\n",
    "\n",
    "## Secondly we remove the common words in our document corpus and tokenize \n",
    "# The re.split function takes as first arguments everything we split at. At the moment, this is \n",
    "# ' ' - '\\' - ''' (apostrophe) -  '\\n' - '(', ')' - ',' - '.' - ':' - ';'\n",
    "# We also filter the words which are shorter than 3 letters, as they are very unlikely to provide any information, \n",
    "# and finally, we remove the common words.\n",
    "texts = [[word for word in re.split(' |\\'|\\n|\\(|\\)|,|;|:|\\.|\\[|\\]|\\â€™',\n",
    "                                    document.lower()) if (len(word) > 4 and (word not in stop_words))] \n",
    "         for document in dataset]\n",
    "\n",
    "# Thirdly we remove the words that appear only once in a text - Consider the stemmed version\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "FS = FrenchStemmer()\n",
    "\n",
    "# NOTE THAT AT THE MOMENT, WE DO NOT DO STEMMING EVEN IF IT IS LOADED\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform the LDA topic modelling and print the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the data into a dictionnary and a corpus, necessary entries for the LdaModel function of Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "# Converts a collection of words to its bag of word representation (list of word_id, word_frequency 2-tuples$)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../datas/lda\"):\n",
    "    os.makedirs(\"../datas/lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary.save('../datas/lda/ldaDictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the algorithm below, we need to choose the number of topics, which is the number of clusters of data that we want to find. Note that the accuracy of our algorithm depends a lot on picking a good number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=11,id2word = dictionary)#, passes=1)\n",
    "#ldamodel = gensim.models.hdpmodel.HdpModel(corpus, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, bag in enumerate(ldamodel.print_topics(num_words=8)):\n",
    "    print(\"============\")\n",
    "    print(\"Cluster \", i, \": \", bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel.save('../datas/lda/ldamodel')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
