{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Topic Modelling using the Gensim Library\n",
    "\n",
    "Usual imports come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import logging\n",
    "import gensim\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data from the Transcript files\n",
    "At the moment, we only consider the entries for which the field `LanguageOfText` is `FR`, namely the ones in French. We will consider the text in German later on. We show below one example of the text we consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "path = '../datas/treated_data/Transcript/'\n",
    "#path = 'datas/Vote/'\n",
    "allFiles = glob.glob(os.path.join(path, 'DE*.csv'))\n",
    "\n",
    "for file_ in allFiles:\n",
    "    data = pd.read_csv(file_)\n",
    "    dataset = dataset + list(data[(data['Text'] == data['Text'])]['Text'].values)\n",
    "    #dataset = dataset + list(data[(data['BusinessTitle'] == data['BusinessTitle'])]['BusinessTitle'].values+' ')\n",
    "\n",
    "    \n",
    "print('Length of the dataset', len(dataset))\n",
    "#print(dataset[0],'\\n',dataset[1])\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the transcripts largely vary from an entry to another, but it reflects exactly what is discussed at the federal parliament. Processing them correctly will allow us to grasp the topic which are discussed at the parliament."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Format the data in order to use LDA with Gensim\n",
    "First of all, we load the `stop_words`, a list which refers all the common words for French, and that we must not take into accoung when doing the topic modelling, as they do not convey any useful information. The pipeline we follow is the following :\n",
    " 1. Load the `stop_words` (using the [stop_words](https://pypi.python.org/pypi/stop-words) package) : We do not load the package as we loaded it once and save the resulting stop words into a .txt file. We do that in order to be able to add some stop words of our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stop_words():\n",
    "    \"\"\"\n",
    "        Loads and concatenates the stop_words list of both french and german languages \n",
    "        (due to the fact that there are some german words in the french transcript and vice-versa)\n",
    "    \"\"\"\n",
    "    #1. Load the custom French stop words dictionary\n",
    "    with open (\"../datas/stop_dictionaries/French_stop_words_changed.txt\", \"r\") as myfile:\n",
    "        stop_words_fr=myfile.read()  \n",
    "    stop_words_fr = stop_words_fr.split(',')\n",
    "\n",
    "    #2. Load the custom German stop words dictionary    \n",
    "    with open (\"../datas/stop_dictionaries/German_stop_words.txt\", \"r\") as myfile:\n",
    "        stop_words_de=myfile.read()  \n",
    "    stop_words_de = stop_words_de.split(', ')\n",
    "    \n",
    "    return stop_words_de+stop_words_fr\n",
    "stop_words = stop_words()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Remove those common words and tokenize our dataset (break it down into words) \n",
    " 3. We count the frequency of the words and remove the ones that appear only once in total.\n",
    " 4. (Implement the *Stemming* of the data (cf. [a French stemming algorithm](http://snowball.tartarus.org/algorithms/french/stemmer.html)). (Done with the [nltk](http://www.nltk.org/api/nltk.stem.html) library) ) -> Not implemented at the moment\n",
    " 5.  Remove all the words of length <= 2.\n",
    " \n",
    " **N.B. THIS ALGORITHM IS VERY SLOW !!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def format_text(dataset, stop_words, stemming = False):\n",
    "    \"\"\"\n",
    "    Here, we remove the common words in our document corpus and tokenize it, before \n",
    "    \"\"\" \n",
    "    # The re.split function takes as first arguments everything we split at. At the moment, this is \n",
    "    # ' ' - '\\'- '/' - ''' (apostrophe) -  '\\n' - '(', ')' - ',' - '.' - ':' - ';' -'[' - ']' and - '´'\n",
    "    # We also filter the words which are shorter than 3 letters, as they are very unlikely \n",
    "    #to provide any information,  and finally, we remove the common words.\n",
    "    \n",
    "    texts = [[word for word in re.split(' |\\'|\\n|\\(|\\)|,|;|:|\\.|\\[|\\]|\\’|\\/',\n",
    "                                    document.lower()) if (len(word) > 4 and (word not in stop_words))] \n",
    "             for document in dataset]\n",
    "\n",
    "    # Thirdly we remove the words that appear only once in a text    \n",
    "    if stemming:\n",
    "        #Consider the stemmed version\n",
    "        FS = FrenchStemmer()\n",
    "\n",
    "        frequency = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[FS.stem(token)] += 1\n",
    "\n",
    "        texts = [[FS.stem(token) for token in text if frequency[FS.stem(token)] > 1]\n",
    "                 for text in texts]\n",
    "    else:\n",
    "        \n",
    "        frequency = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[token] += 1\n",
    "\n",
    "\n",
    "        texts = [[token for token in text if frequency[token] > 1]\n",
    "                 for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = format_text(dataset,stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform the LDA topic modelling and print the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the data into a dictionnary and a corpus, necessary entries for the LdaModel function of Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "# Converts a collection of words to its bag of word representation (list of word_id, word_frequency 2-tuples$)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../datas/lda\"):\n",
    "    os.makedirs(\"../datas/lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary.save('../datas/lda/ldaDictionaryDE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the algorithm below, we need to choose the number of topics, which is the number of clusters of data that we want to find. Note that the accuracy of our algorithm depends a lot on picking a good number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=11,id2word = dictionary)#, passes=1)\n",
    "#ldamodel = gensim.models.hdpmodel.HdpModel(corpus, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, bag in enumerate(ldamodel.print_topics(num_words=8)):\n",
    "    print(\"============\")\n",
    "    print(\"Cluster \", i, \": \", bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel.save('../datas/lda/ldamodelDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
